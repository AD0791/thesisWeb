[
  {
    "objectID": "maxlag.html",
    "href": "maxlag.html",
    "title": "Etablissement du nombre de retard Maximal pour les series sous etudes",
    "section": "",
    "text": "Les critères d’information sont des outils utilisés en statistiques pour sélectionner un modèle parmi un ensemble de modèles candidats. L’objectif est de choisir le modèle qui offre un bon équilibre entre l’ajustement aux données et la complexité du modèle. Les deux critères d’information les plus couramment utilisés sont le Critère d’Information Akaike (AIC) et le Critère d’Information Bayésien (BIC). Voici une explication de ces deux critères :\n\nCritère d’Information Akaike (AIC) :\n\nLe Critère d’Information Akaike (AIC) a été développé par Hirotugu Akaike dans les années 1970. Il est basé sur la théorie de l’information et est largement utilisé pour comparer des modèles statistiques.\nL’AIC est calculé à partir de la fonction de vraisemblance du modèle et de sa complexité, mesurée par le nombre de paramètres du modèle. L’AIC est défini comme : \\[\\text{AIC} = -2 \\times \\text{log-vraisemblance maximisée} + 2 \\times \\text{nombre de paramètres du modèle}\\]\nLe modèle avec le plus faible AIC est considéré comme le meilleur ajustement parmi les modèles considérés.\n\nCritère d’Information Bayésien (BIC) :\n\nLe Critère d’Information Bayésien (BIC), également connu sous le nom de Critère de Schwarz, a été développé par Gideon Schwarz dans les années 1970. Il est basé sur la théorie bayésienne de la probabilité.\nComme l’AIC, le BIC prend en compte à la fois l’ajustement aux données et la complexité du modèle. Cependant, le BIC pénalise la complexité du modèle plus fortement que l’AIC.\nLe BIC est calculé comme suit : \\[\\text{BIC} = -2 \\times \\text{log-vraisemblance maximisée} + \\log(n) \\times \\text{nombre de paramètres du modèle}\\] où $ n $ est la taille de l’échantillon.\nComme pour l’AIC, le modèle avec le plus faible BIC est considéré comme le meilleur ajustement parmi les modèles considérés.\n\n\nEn résumé, les critères d’information, tels que l’AIC et le BIC, fournissent une approche objective pour comparer différents modèles statistiques en tenant compte à la fois de leur ajustement aux données et de leur complexité. Ces critères sont largement utilisés dans la sélection de modèles en statistiques et en apprentissage automatique."
  },
  {
    "objectID": "maxlag.html#information-criterion",
    "href": "maxlag.html#information-criterion",
    "title": "Etablissement du nombre de retard Maximal pour les series sous etudes",
    "section": "",
    "text": "Les critères d’information sont des outils utilisés en statistiques pour sélectionner un modèle parmi un ensemble de modèles candidats. L’objectif est de choisir le modèle qui offre un bon équilibre entre l’ajustement aux données et la complexité du modèle. Les deux critères d’information les plus couramment utilisés sont le Critère d’Information Akaike (AIC) et le Critère d’Information Bayésien (BIC). Voici une explication de ces deux critères :\n\nCritère d’Information Akaike (AIC) :\n\nLe Critère d’Information Akaike (AIC) a été développé par Hirotugu Akaike dans les années 1970. Il est basé sur la théorie de l’information et est largement utilisé pour comparer des modèles statistiques.\nL’AIC est calculé à partir de la fonction de vraisemblance du modèle et de sa complexité, mesurée par le nombre de paramètres du modèle. L’AIC est défini comme : \\[\\text{AIC} = -2 \\times \\text{log-vraisemblance maximisée} + 2 \\times \\text{nombre de paramètres du modèle}\\]\nLe modèle avec le plus faible AIC est considéré comme le meilleur ajustement parmi les modèles considérés.\n\nCritère d’Information Bayésien (BIC) :\n\nLe Critère d’Information Bayésien (BIC), également connu sous le nom de Critère de Schwarz, a été développé par Gideon Schwarz dans les années 1970. Il est basé sur la théorie bayésienne de la probabilité.\nComme l’AIC, le BIC prend en compte à la fois l’ajustement aux données et la complexité du modèle. Cependant, le BIC pénalise la complexité du modèle plus fortement que l’AIC.\nLe BIC est calculé comme suit : \\[\\text{BIC} = -2 \\times \\text{log-vraisemblance maximisée} + \\log(n) \\times \\text{nombre de paramètres du modèle}\\] où $ n $ est la taille de l’échantillon.\nComme pour l’AIC, le modèle avec le plus faible BIC est considéré comme le meilleur ajustement parmi les modèles considérés.\n\n\nEn résumé, les critères d’information, tels que l’AIC et le BIC, fournissent une approche objective pour comparer différents modèles statistiques en tenant compte à la fois de leur ajustement aux données et de leur complexité. Ces critères sont largement utilisés dans la sélection de modèles en statistiques et en apprentissage automatique."
  },
  {
    "objectID": "maxlag.html#max-lag-importation",
    "href": "maxlag.html#max-lag-importation",
    "title": "Etablissement du nombre de retard Maximal pour les series sous etudes",
    "section": "Max Lag Importation",
    "text": "Max Lag Importation"
  },
  {
    "objectID": "maxlag.html#max-lag-exportation",
    "href": "maxlag.html#max-lag-exportation",
    "title": "Etablissement du nombre de retard Maximal pour les series sous etudes",
    "section": "Max Lag Exportation",
    "text": "Max Lag Exportation"
  },
  {
    "objectID": "cusum_test.html",
    "href": "cusum_test.html",
    "title": "Test de Stabilité CUSUM",
    "section": "",
    "text": "Le test CUSUM (Cumulative Sum) est un outil d’analyse de la stabilité des paramètres dans un modèle de régression. Il a été introduit par Brown, Durbin et Evans (1975) pour détecter les changements structurels dans les paramètres d’un modèle. L’idée principale du test CUSUM est de voir si les résidus récursifs s’éloignent de manière significative de leur moyenne de zéro.\nLe test CUSUM est basé sur la somme cumulée des résidus récursifs. La statistique du test est calculée comme suit:\n\\(W_t = \\sum_{i=k+1}^{t} \\frac{w_i}{\\hat{\\sigma}}\\) pour \\(t = k+1, ..., T\\)\noù: - \\(w_i\\) sont les résidus récursifs. - \\(\\hat{\\sigma}\\) est l’estimation de l’écart-type des résidus récursifs.\nLa statistique \\(W_t\\) est ensuite normalisée et tracée par rapport au temps. Si la statistique CUSUM se situe à l’intérieur d’une bande de confiance, on ne rejette pas l’hypothèse nulle de la stabilité des paramètres. Si la statistique CUSUM sort de la bande de confiance, on rejette l’hypothèse nulle et on conclut à une instabilité des paramètres.\nLes lignes de confiance sont calculées comme suit:\n\\(y = \\pm [a\\sqrt{(T-k)} + \\frac{2a(t-k)}{\\sqrt{(T-k)}}]\\)\noù: - \\(a\\) est une constante qui dépend du niveau de signification souhaité (par exemple, a = 0,948 pour un niveau de signification de 5%). - \\(T\\) est le nombre total d’observations. - \\(k\\) est le nombre de paramètres dans le modèle. - \\(t\\) est le point temporel actuel."
  },
  {
    "objectID": "cusum_test.html#cusum-it",
    "href": "cusum_test.html#cusum-it",
    "title": "Test de Stabilité CUSUM",
    "section": "CUSUM IT",
    "text": "CUSUM IT"
  },
  {
    "objectID": "cusum_test.html#cusum-et",
    "href": "cusum_test.html#cusum-et",
    "title": "Test de Stabilité CUSUM",
    "section": "CUSUM ET",
    "text": "CUSUM ET"
  },
  {
    "objectID": "cointegration_test_it.html",
    "href": "cointegration_test_it.html",
    "title": "Test de Cointégration (Modèle IT)",
    "section": "",
    "text": "Le test de cointégration de Pesaran, également connu sous le nom de “Bound test”, est utilisé pour déterminer s’il existe une relation de long terme entre les variables d’un modèle ARDL.\n\n\nLe test est basé sur un test de Wald (test F) des coefficients des termes de long terme.\n\nH0 : Il n’y a pas de relation de long terme (pas de cointégration).\nH1 : Il existe une relation de long terme (cointégration).\n\n\n\n\nLa statistique F calculée est comparée à deux bornes critiques :\n\nSi la statistique F est supérieure à la borne supérieure, on rejette H0 et on conclut à la cointégration.\nSi la statistique F est inférieure à la borne inférieure, on ne rejette pas H0.\nSi la statistique F se situe entre les deux bornes, le test est non concluant.\n\n\n\n\nPour le modèle d’importation (IT), le cas 4 (intercept non restreint et tendance restreinte) a été retenu."
  },
  {
    "objectID": "cointegration_test_it.html#test-de-cointégration-de-pesaran-bound-test-pour-le-modèle-it",
    "href": "cointegration_test_it.html#test-de-cointégration-de-pesaran-bound-test-pour-le-modèle-it",
    "title": "Test de Cointégration (Modèle IT)",
    "section": "",
    "text": "Le test de cointégration de Pesaran, également connu sous le nom de “Bound test”, est utilisé pour déterminer s’il existe une relation de long terme entre les variables d’un modèle ARDL.\n\n\nLe test est basé sur un test de Wald (test F) des coefficients des termes de long terme.\n\nH0 : Il n’y a pas de relation de long terme (pas de cointégration).\nH1 : Il existe une relation de long terme (cointégration).\n\n\n\n\nLa statistique F calculée est comparée à deux bornes critiques :\n\nSi la statistique F est supérieure à la borne supérieure, on rejette H0 et on conclut à la cointégration.\nSi la statistique F est inférieure à la borne inférieure, on ne rejette pas H0.\nSi la statistique F se situe entre les deux bornes, le test est non concluant.\n\n\n\n\nPour le modèle d’importation (IT), le cas 4 (intercept non restreint et tendance restreinte) a été retenu."
  },
  {
    "objectID": "granger_causality.html",
    "href": "granger_causality.html",
    "title": "Test de Causalité de Granger",
    "section": "",
    "text": "Le test de causalité de Granger est un test statistique permettant de déterminer si une série temporelle est utile pour prévoir une autre. L’hypothèse nulle est que les coefficients des valeurs passées de la première série temporelle sont nuls dans une régression des valeurs de la deuxième série temporelle sur ses propres valeurs passées et sur les valeurs passées de la première série temporelle. Si la valeur p est inférieure à un certain niveau de signification, l’hypothèse nulle est rejetée et on conclut que la première série temporelle est utile pour prévoir la seconde.\nLe test de causalité de Granger est basé sur les régressions suivantes:\nModèle non restreint: \\(Y_t = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i Y_{t-i} + \\sum_{j=1}^{q} \\beta_j X_{t-j} + \\epsilon_t\\)\nModèle restreint: \\(Y_t = \\alpha_0 + \\sum_{i=1}^{p} \\alpha_i Y_{t-i} + u_t\\)\noù: - \\(Y_t\\) est la valeur de la série temporelle Y à l’instant t. - \\(X_t\\) est la valeur de la série temporelle X à l’instant t. - \\(\\alpha_i\\) et \\(\\beta_j\\) sont les coefficients de régression. - \\(\\epsilon_t\\) et \\(u_t\\) sont les termes d’erreur. - \\(p\\) et \\(q\\) sont les ordres de retard.\nL’hypothèse nulle est que \\(X\\) ne cause pas \\(Y\\) au sens de Granger, c’est-à-dire \\(\\beta_1 = \\beta_2 = ... = \\beta_q = 0\\).\nLe test F est utilisé pour tester cette hypothèse. La statistique F est calculée comme suit:\n\\(F = \\frac{(SSR_r - SSR_{ur})/q}{SSR_{ur}/(n-p-q-1)}\\)\noù: - \\(SSR_r\\) est la somme des carrés des résidus du modèle restreint. - \\(SSR_{ur}\\) est la somme des carrés des résidus du modèle non restreint. - \\(n\\) est le nombre d’observations. - \\(p\\) et \\(q\\) sont les ordres de retard."
  },
  {
    "objectID": "granger_causality.html#test-de-causalite-de-ganger",
    "href": "granger_causality.html#test-de-causalite-de-ganger",
    "title": "Test de Causalité de Granger",
    "section": "Test de Causalite de Ganger",
    "text": "Test de Causalite de Ganger"
  },
  {
    "objectID": "cointegration_test_et.html",
    "href": "cointegration_test_et.html",
    "title": "Test de Cointégration (Modèle ET)",
    "section": "",
    "text": "Le test de cointégration de Pesaran, également connu sous le nom de “Bound test”, est utilisé pour déterminer s’il existe une relation de long terme entre les variables d’un modèle ARDL.\n\n\nLe test est basé sur un test de Wald (test F) des coefficients des termes de long terme.\n\nH0 : Il n’y a pas de relation de long terme (pas de cointégration).\nH1 : Il existe une relation de long terme (cointégration).\n\n\n\n\nLa statistique F calculée est comparée à deux bornes critiques :\n\nSi la statistique F est supérieure à la borne supérieure, on rejette H0 et on conclut à la cointégration.\nSi la statistique F est inférieure à la borne inférieure, on ne rejette pas H0.\nSi la statistique F se situe entre les deux bornes, le test est non concluant.\n\n\n\n\nPour le modèle d’exportation (ET), le cas 3 (intercept non restreint et sans tendance) a été retenu."
  },
  {
    "objectID": "cointegration_test_et.html#test-de-cointégration-de-pesaran-bound-test-pour-le-modèle-et",
    "href": "cointegration_test_et.html#test-de-cointégration-de-pesaran-bound-test-pour-le-modèle-et",
    "title": "Test de Cointégration (Modèle ET)",
    "section": "",
    "text": "Le test de cointégration de Pesaran, également connu sous le nom de “Bound test”, est utilisé pour déterminer s’il existe une relation de long terme entre les variables d’un modèle ARDL.\n\n\nLe test est basé sur un test de Wald (test F) des coefficients des termes de long terme.\n\nH0 : Il n’y a pas de relation de long terme (pas de cointégration).\nH1 : Il existe une relation de long terme (cointégration).\n\n\n\n\nLa statistique F calculée est comparée à deux bornes critiques :\n\nSi la statistique F est supérieure à la borne supérieure, on rejette H0 et on conclut à la cointégration.\nSi la statistique F est inférieure à la borne inférieure, on ne rejette pas H0.\nSi la statistique F se situe entre les deux bornes, le test est non concluant.\n\n\n\n\nPour le modèle d’exportation (ET), le cas 3 (intercept non restreint et sans tendance) a été retenu."
  },
  {
    "objectID": "normality_test.html",
    "href": "normality_test.html",
    "title": "Test de Normalité de Shapiro-Wilk",
    "section": "",
    "text": "Le test de Shapiro-Wilk est un test de normalité dans les statistiques fréquentistes. Il a été publié en 1965 par Samuel Sanford Shapiro et Martin Wilk.\nL’hypothèse nulle du test est que les données sont distribuées normalement. Si la valeur p est inférieure à un certain niveau de signification (généralement 0,05), l’hypothèse nulle est rejetée et on conclut que les données ne sont pas distribuées normalement.\nLa statistique du test est:\n\\(W = \\frac{(\\sum_{i=1}^{n} a_i x_{(i)})^2}{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}\\)\noù: - \\(x_{(i)}\\) est la i-ème statistique d’ordre (c’est-à-dire le i-ème plus petit nombre dans l’échantillon). - \\(\\bar{x}\\) est la moyenne de l’échantillon. - les coefficients \\(a_i\\) sont donnés par \\(a = (a_1, ..., a_n) = \\frac{m^T V^{-1}}{(m^T V^{-1} V^{-1} m)^{1/2}}\\) - où \\(m = (m_1, ..., m_n)^T\\) sont les valeurs attendues des statistiques d’ordre d’un échantillon de variables aléatoires indépendantes et identiquement distribuées selon une loi normale, et \\(V\\) est la matrice de covariance de ces statistiques d’ordre."
  },
  {
    "objectID": "normality_test.html#test-de-normalite-des-series-dinteret",
    "href": "normality_test.html#test-de-normalite-des-series-dinteret",
    "title": "Test de Normalité de Shapiro-Wilk",
    "section": "test de Normalite des series d’interet",
    "text": "test de Normalite des series d’interet"
  },
  {
    "objectID": "cointegration_test.html",
    "href": "cointegration_test.html",
    "title": "Test de Cointégration de Pesaran (Bound Test)",
    "section": "",
    "text": "Considérons le modèle ARDL suivant :\n\\[ \\Delta Y_t = \\alpha + \\beta_1 Y_{t-1} + \\beta_2 \\Delta Y_{t-1} + \\beta_3 \\Delta Y_{t-2} + \\ldots + \\beta_p \\Delta Y_{t-p} + \\gamma_1 X_{1,t-1} + \\gamma_2 \\Delta X_{1,t-1} + \\ldots + \\gamma_q \\Delta X_{1,t-q} + \\epsilon_t \\]\nLe ARDL Bound Test implique deux tests de nullité :"
  },
  {
    "objectID": "cointegration_test.html#test-de-cointégration-de-pesaran-bound-test---modèle-it",
    "href": "cointegration_test.html#test-de-cointégration-de-pesaran-bound-test---modèle-it",
    "title": "Test de Cointégration de Pesaran (Bound Test)",
    "section": "Test de cointégration de Pesaran (Bound test) - Modèle IT",
    "text": "Test de cointégration de Pesaran (Bound test) - Modèle IT\nPesaran et al. [12] define five different cases according to the incorporation of intercept (μ0) and trend (μ1) coefficients in the error correction term. Case 1: No intercept and no trend. Case 2: Restricted intercept and no trend. Case 3: Unrestricted intercept and no trend. Case 4: Unrestricted intercept and restricted trend. Case 5: Unrestricted intercept and unrestricted trend.\n\n4ème cas retenu"
  },
  {
    "objectID": "cointegration_test.html#test-de-cointégration-de-pesaran-bound-test---modèle-et",
    "href": "cointegration_test.html#test-de-cointégration-de-pesaran-bound-test---modèle-et",
    "title": "Test de Cointégration de Pesaran (Bound Test)",
    "section": "Test de cointégration de Pesaran (Bound test) - Modèle ET",
    "text": "Test de cointégration de Pesaran (Bound test) - Modèle ET\nPesaran et al. [12] define five different cases according to the incorporation of intercept (μ0) and trend (μ1) coefficients in the error correction term. Case 1: No intercept and no trend. Case 2: Restricted intercept and no trend. Case 3: Unrestricted intercept and no trend. Case 4: Unrestricted intercept and restricted trend. Case 5: Unrestricted intercept and unrestricted trend.\n\n3ème cas retenu"
  },
  {
    "objectID": "cointegration_test.html#conclusion-du-test-de-cointégration",
    "href": "cointegration_test.html#conclusion-du-test-de-cointégration",
    "title": "Test de Cointégration de Pesaran (Bound Test)",
    "section": "Conclusion du test de cointégration",
    "text": "Conclusion du test de cointégration\n\nModèle IT:\n\nECM avec intercept non restreint et une tendance restreinte\n\nModèle ET:\n\nECM avec intercept non restreint et sans tendance"
  },
  {
    "objectID": "stationarity.html",
    "href": "stationarity.html",
    "title": "Analyse de la Stationnarité",
    "section": "",
    "text": "La procédure d’étude de la stationnarité demande que l’on procède à des tests de stationnarité pour les séries en niveau. Ensuite on passe le filtre de différence sur les séries et on recommence les tests."
  },
  {
    "objectID": "stationarity.html#procédure",
    "href": "stationarity.html#procédure",
    "title": "Analyse de la Stationnarité",
    "section": "",
    "text": "La procédure d’étude de la stationnarité demande que l’on procède à des tests de stationnarité pour les séries en niveau. Ensuite on passe le filtre de différence sur les séries et on recommence les tests."
  },
  {
    "objectID": "stationarity.html#filtre-de-différence",
    "href": "stationarity.html#filtre-de-différence",
    "title": "Analyse de la Stationnarité",
    "section": "Filtre de différence",
    "text": "Filtre de différence\nLe filtre de différence est une opération couramment utilisée dans l’analyse des séries chronologiques pour transformer une série en une série stationnaire en différenciant les observations. La différenciation implique de soustraire chaque observation de la série par son observation précédente. Cette opération aide à éliminer les tendances et les structures temporelles de la série, rendant ainsi la série stationnaire. Voici comment le filtre de différence est mathématiquement représenté :\nSi nous avons une série chronologique (y_t) pour (t = 1, 2, …, T), alors la série de différences premières, notée (y_t), est définie comme :\n\\[ \\Delta y_t = y_t - y_{t-1} \\]\nCette équation montre que chaque observation (y_t) est soustraite de son observation précédente (y_{t-1}) pour obtenir la différence première (y_t). Cela peut être répété pour chaque observation dans la série, créant ainsi une nouvelle série de différences premières avec une longueur de (T - 1), car la première observation n’a pas de valeur précédente à soustraire.\nL’opération de différenciation peut être répétée plusieurs fois si nécessaire pour obtenir une série encore plus stationnaire, en soustrayant chaque observation par son observation précédente dans la série de différences premières. La série résultante est appelée série de différences d’ordre (d), où (d) représente le nombre de différences effectuées.\nVoici un exemple d’équation LaTeX pour représenter la série de différences premières : Cette équation peut être utilisée dans les documents LaTeX pour représenter mathématiquement le concept de différenciation dans l’analyse des séries chronologiques."
  },
  {
    "objectID": "stationarity.html#hypothèses-pour-les-tests-de-stationnarité",
    "href": "stationarity.html#hypothèses-pour-les-tests-de-stationnarité",
    "title": "Analyse de la Stationnarité",
    "section": "Hypothèses pour les tests de stationnarité",
    "text": "Hypothèses pour les tests de stationnarité\nBien sûr, voici les hypothèses nulles et alternatives pour les tests ADF, KPSS et Phillips-Perron :\n\nTest de Dickey-Fuller Augmenté (ADF) :\n\nHypothèse Nulle (H0) : La série chronologique possède une racine unitaire, ce qui signifie qu’elle n’est pas stationnaire.\nHypothèse Alternative (H1) : La série chronologique ne possède pas de racine unitaire, ce qui signifie qu’elle est stationnaire.\n\nTest KPSS (Kwiatkowski-Phillips-Schmidt-Shin) :\n\nHypothèse Nulle (H0) : La série chronologique est stationnaire.\nHypothèse Alternative (H1) : La série chronologique n’est pas stationnaire.\n\nTest Phillips-Perron :\n\nHypothèse Nulle (H0) : La série chronologique possède une racine unitaire, ce qui signifie qu’elle n’est pas stationnaire.\nHypothèse Alternative (H1) : La série chronologique ne possède pas de racine unitaire, ce qui signifie qu’elle est stationnaire.\n\n\nIl est important de noter que les conclusions des tests dépendent de la p-value associée. Si la p-value est inférieure à un certain seuil (généralement 0,05), on rejette l’hypothèse nulle au profit de l’hypothèse alternative, ce qui signifie que la série est considérée comme stationnaire. Sinon, si la p-value est supérieure au seuil, on ne peut pas rejeter l’hypothèse nulle, ce qui indique que la série n’est pas stationnaire."
  },
  {
    "objectID": "stationarity.html#tableau-présentant-les-résultats-des-tests-de-stationnarité",
    "href": "stationarity.html#tableau-présentant-les-résultats-des-tests-de-stationnarité",
    "title": "Analyse de la Stationnarité",
    "section": "Tableau Présentant les résultats des tests de stationnarité",
    "text": "Tableau Présentant les résultats des tests de stationnarité"
  },
  {
    "objectID": "ET_modele.html",
    "href": "ET_modele.html",
    "title": "Modele ET",
    "section": "",
    "text": "representation classique\nrepresantation short run et long run"
  },
  {
    "objectID": "ET_modele.html#presentation-des-resultats-du-modele-et",
    "href": "ET_modele.html#presentation-des-resultats-du-modele-et",
    "title": "Modele ET",
    "section": "",
    "text": "representation classique\nrepresantation short run et long run"
  },
  {
    "objectID": "ET_modele.html#representation-classique",
    "href": "ET_modele.html#representation-classique",
    "title": "Modele ET",
    "section": "representation classique",
    "text": "representation classique\n\n\n\n\n\n\n\n\nModèle à Correction d'Erreur (ECM)\n\n\nCoefficients et statistiques\n\n\nTerme\nErreur std\nt-value\np-value\nIC 95% inf\nIC 95% sup\nCoefficient\n\n\n\n\n(Intercept)\n36.0759\n5.4083\n5.80e-05\n118.6317\n271.5867\n195.1092 ***\n\n\nec.1\n0.3084\n-5.4036\n5.85e-05\n-2.3200\n-1.0126\n-1.6663 ***\n\n\ndtaux.change.t\n0.4254\n-3.1760\n0.00587\n-2.2530\n-0.4493\n-1.3511 **\n\n\ndtaux.change.1\n0.3930\n2.7701\n0.01366\n0.2556\n1.9220\n1.0888 *\n\n\ndtaux.change.2\n0.5014\n3.1189\n0.00661\n0.5009\n2.6266\n1.5638 **\n\n\ndpib.t\n1.6448\n7.2855\n1.83e-06\n8.4965\n15.4702\n11.9834 ***\n\n\ndpib.1\n1.6022\n-1.0963\n0.28916\n-5.1530\n1.6399\n-1.7565\n\n\ndpib.usa.t\n2.4494\n-0.5244\n0.60722\n-6.4769\n3.9082\n-1.2844\n\n\ndpib.usa.1\n3.4964\n3.1523\n0.00617\n3.6095\n18.4336\n11.0216 **\n\n\ndpib.usa.2\n4.3684\n2.1001\n0.05193\n-0.0863\n18.4351\n9.1744 .\n\n\ndpib.usa.3\n3.4294\n-2.1795\n0.04458\n-14.7445\n-0.2045\n-7.4745 *\n\n\ndipc.t\n1.0779\n3.3160\n0.00437\n1.2892\n5.8592\n3.5742 **\n\n\ndipc.1\n1.0346\n-3.2642\n0.00487\n-5.5704\n-1.1839\n-3.3771 **\n\n\ndexportation.1\n0.1982\n1.8194\n0.08762\n-0.0596\n0.7808\n0.3606 .\n\n\ndexportation.2\n0.1217\n0.6976\n0.49545\n-0.1731\n0.3429\n0.0849\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistiques globales du modèle\n\n\nStatistique\nValeur\n\n\n\n\nR² ajusté\n0.7109\n\n\nF-statistique\n6.2687\n\n\np-value (F)\n3.983e-04\n\n\nLog-vraisemblance\n16.521\n\n\nAIC\n-1.042\n\n\nBIC\n21.9018\n\n\nDegrés de liberté\n14\n\n\nNombre d'observations\n31\n\n\n\n\n\n\n\nVoici la représentation mathématique du modèle à correction d’erreur pour les exportations :\n\\[\n\\begin{align}\n\\Delta \\text{Exportation}_t &= 195.1092^{***} - 1.6663^{***} \\cdot \\text{EC}_{t-1} - 1.3511^{**} \\cdot \\Delta \\text{Taux.Change}_t \\\\\n&+ 1.0888^{*} \\cdot \\Delta \\text{Taux.Change}_{t-1} + 1.5638^{**} \\cdot \\Delta \\text{Taux.Change}_{t-2} \\\\\n&+ 11.9834^{***} \\cdot \\Delta \\text{PIB}_t - 1.7565 \\cdot \\Delta \\text{PIB}_{t-1} \\\\\n&- 1.2844 \\cdot \\Delta \\text{PIB.USA}_t + 11.0216^{**} \\cdot \\Delta \\text{PIB.USA}_{t-1} + 9.1744^{.} \\cdot \\Delta \\text{PIB.USA}_{t-2} - 7.4745^{*} \\cdot \\Delta \\text{PIB.USA}_{t-3} \\\\\n&+ 3.5742^{**} \\cdot \\Delta \\text{IPC}_t - 3.3771^{**} \\cdot \\Delta \\text{IPC}_{t-1} \\\\\n&+ 0.3606^{.} \\cdot \\Delta \\text{Exportation}_{t-1} + 0.0849 \\cdot \\Delta \\text{Exportation}_{t-2} + \\varepsilon_t\n\\end{align}\n\\]\nOù \\(\\text{EC}_{t-1}\\) est le terme de correction d’erreur défini comme :\n\\[\n\\begin{align}\n\\text{EC}_{t-1} &= \\text{Exportation}_{t-1} + 1.5866 \\cdot \\text{Taux.Change}_{t-1} - 12.6797^{***} \\cdot \\text{PIB}_{t-1} \\\\\n&+ 15.814^{**} \\cdot \\text{PIB.USA}_{t-1} - 3.5121^{**} \\cdot \\text{IPC}_{t-1}\n\\end{align}\n\\]"
  },
  {
    "objectID": "ET_modele.html#avec-long-run",
    "href": "ET_modele.html#avec-long-run",
    "title": "Modele ET",
    "section": "Avec long run",
    "text": "Avec long run\n\n\n\n\n\n\n\n\nCoefficients de long terme\n\n\nErreur std\nt-value\np-value\nCoefficient\nVariable\n\n\n\n\n0.3329\n−5.3565\n0.00017\n-1.7831 ***\nExportation (t-1)\n\n\n1.1360\n−1.3967\n0.18779\n-1.5866\nTaux de change (t-1)\n\n\n2.6717\n4.7460\n0.00048\n12.6797 ***\nPIB Haïti (t-1)\n\n\n4.8073\n−3.2896\n0.00646\n-15.814 **\nPIB USA (t-1)\n\n\n0.8657\n4.0570\n0.00159\n3.5121 **\nIPC (t-1)\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n\n\n\n\n\n\n\n\nLa relation de long terme pour le modèle d’exportation est :\n\\[\n\\begin{align}\n\\text{Exportation}_t &= -1.5866 \\cdot \\text{Taux.Change}_t + 12.6797^{***} \\cdot \\text{PIB}_t - 15.814^{**} \\cdot \\text{PIB.USA}_t + 3.5121^{**} \\cdot \\text{IPC}_t + \\text{constante}\n\\end{align}\n\\]"
  },
  {
    "objectID": "cusum_test_it.html",
    "href": "cusum_test_it.html",
    "title": "Test CUSUM (Modèle IT)",
    "section": "",
    "text": "Le test CUSUM (Cumulative Sum) est un test de stabilité des paramètres du modèle. Il est basé sur la somme cumulative des résidus récursifs.\n\n\nSi la ligne CUSUM reste dans les limites de confiance (les deux lignes rouges), les paramètres du modèle sont considérés comme stables dans le temps."
  },
  {
    "objectID": "cusum_test_it.html#test-cusum-pour-le-modèle-it",
    "href": "cusum_test_it.html#test-cusum-pour-le-modèle-it",
    "title": "Test CUSUM (Modèle IT)",
    "section": "",
    "text": "Le test CUSUM (Cumulative Sum) est un test de stabilité des paramètres du modèle. Il est basé sur la somme cumulative des résidus récursifs.\n\n\nSi la ligne CUSUM reste dans les limites de confiance (les deux lignes rouges), les paramètres du modèle sont considérés comme stables dans le temps."
  },
  {
    "objectID": "IT_modele.html",
    "href": "IT_modele.html",
    "title": "Modele IT",
    "section": "",
    "text": "representation short run et long run\nGet the long rung"
  },
  {
    "objectID": "IT_modele.html#presentation-des-resultats-du-modele-it",
    "href": "IT_modele.html#presentation-des-resultats-du-modele-it",
    "title": "Modele IT",
    "section": "",
    "text": "representation short run et long run\nGet the long rung"
  },
  {
    "objectID": "IT_modele.html#representation-classique",
    "href": "IT_modele.html#representation-classique",
    "title": "Modele IT",
    "section": "Representation Classique",
    "text": "Representation Classique\n\n\n\n\n\n\n\n\nModèle à Correction d'Erreur (ECM)\n\n\nCoefficients et statistiques\n\n\nTerme\nErreur std\nt-value\np-value\nIC 95% inf\nIC 95% sup\nCoefficient\n\n\n\n\n(Intercept)\n9.9361\n-5.0827\n0.000167\n-71.8139\n-29.1920\n-50.503 ***\n\n\nec.1\n0.2446\n-5.0800\n0.000168\n-1.7674\n-0.7180\n-1.2427 ***\n\n\ndtaux.change.t\n0.2042\n-3.8344\n0.001823\n-1.2210\n-0.3450\n-0.783 **\n\n\ndtaux.change.1\n0.2563\n5.3758\n9.78e-05\n0.8281\n1.9275\n1.3778 ***\n\n\ndtaux.change.2\n0.3046\n2.7457\n0.015779\n0.1830\n1.4897\n0.8364 *\n\n\ndtaux.change.3\n0.2637\n4.0660\n0.001157\n0.5067\n1.6380\n1.0723 **\n\n\ndpib.t\n0.7345\n1.9602\n0.070183\n-0.1356\n3.0150\n1.4397 .\n\n\ndpib.1\n0.7377\n1.4272\n0.175441\n-0.5294\n2.6351\n1.0529\n\n\ndpib.usa.t\n1.2399\n0.7178\n0.484697\n-1.7694\n3.5493\n0.89\n\n\ndpib.usa.1\n1.2806\n-1.0581\n0.307907\n-4.1016\n1.3915\n-1.355\n\n\ndpib.usa.2\n1.4518\n-1.1891\n0.254167\n-4.8403\n1.3875\n-1.7264\n\n\ndpib.usa.3\n1.5072\n1.8850\n0.080352\n-0.3915\n6.0737\n2.8411 .\n\n\ndipc.t\n0.5814\n1.4370\n0.172700\n-0.4116\n2.0826\n0.8355\n\n\ndipc.1\n0.4612\n-3.2818\n0.005457\n-2.5026\n-0.5243\n-1.5135 **\n\n\ndimportation.1\n0.1475\n0.7917\n0.441766\n-0.1995\n0.4330\n0.1167\n\n\ndimportation.2\n0.1455\n1.6705\n0.117019\n-0.0690\n0.5552\n0.2431\n\n\ndimportation.3\n0.1410\n2.7793\n0.014768\n0.0895\n0.6945\n0.392 *\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistiques globales du modèle\n\n\nStatistique\nValeur\n\n\n\n\nR² ajusté\n0.6352\n\n\nF-statistique\n4.2653\n\n\np-value (F)\n0.004646\n\n\nLog-vraisemblance\n42.1279\n\n\nAIC\n-48.2557\n\n\nBIC\n-22.444\n\n\nDegrés de liberté\n16\n\n\nNombre d'observations\n31\n\n\n\n\n\n\n\nVoici la représentation mathématique du modèle à correction d’erreur pour les importations :\n\\[\n\\begin{align}\n\\Delta \\text{Importation}_t &= -50.503^{***} - 1.2427^{***} \\cdot \\text{EC}_{t-1} - 0.783^{**} \\cdot \\Delta \\text{Taux.Change}_t \\\\\n&+ 1.3778^{***} \\cdot \\Delta \\text{Taux.Change}_{t-1} + 0.8364^{*} \\cdot \\Delta \\text{Taux.Change}_{t-2} + 1.0723^{**} \\cdot \\Delta \\text{Taux.Change}_{t-3} \\\\\n&+ 1.4397^{.} \\cdot \\Delta \\text{PIB}_t + 1.0529 \\cdot \\Delta \\text{PIB}_{t-1} \\\\\n&+ 0.89 \\cdot \\Delta \\text{PIB.USA}_t - 1.355 \\cdot \\Delta \\text{PIB.USA}_{t-1} - 1.7264 \\cdot \\Delta \\text{PIB.USA}_{t-2} + 2.8411^{.} \\cdot \\Delta \\text{PIB.USA}_{t-3} \\\\\n&+ 0.8355 \\cdot \\Delta \\text{IPC}_t - 1.5135^{**} \\cdot \\Delta \\text{IPC}_{t-1} \\\\\n&+ 0.1167 \\cdot \\Delta \\text{Importation}_{t-1} + 0.2431 \\cdot \\Delta \\text{Importation}_{t-2} + 0.392^{*} \\cdot \\Delta \\text{Importation}_{t-3} + \\varepsilon_t\n\\end{align}\n\\]\nOù \\(\\text{EC}_{t-1}\\) est le terme de correction d’erreur défini comme :\n\\[\n\\begin{align}\n\\text{EC}_{t-1} &= \\text{Importation}_{t-1} + 0.5926 \\cdot \\text{Taux.Change}_{t-1} - 0.3108 \\cdot \\text{PIB}_{t-1} \\\\\n&- 1.8199 \\cdot \\text{PIB.USA}_{t-1} - 1.0459 \\cdot \\text{IPC}_{t-1}\n\\end{align}\n\\]"
  },
  {
    "objectID": "IT_modele.html#avec-long-run",
    "href": "IT_modele.html#avec-long-run",
    "title": "Modele IT",
    "section": "Avec long run",
    "text": "Avec long run\n\n\n\n\n\n\n\n\nCoefficients de long terme\n\n\nErreur std\nt-value\np-value\nCoefficient\nVariable\n\n\n\n\n0.3052\n−3.9640\n0.00328\n-1.21 **\nImportation (t-1)\n\n\n0.7419\n−0.7987\n0.44501\n-0.5926\nTaux de change (t-1)\n\n\n1.6372\n0.1898\n0.85365\n0.3108\nPIB Haïti (t-1)\n\n\n2.5243\n0.7209\n0.48925\n1.8199\nPIB USA (t-1)\n\n\n0.8176\n1.2793\n0.23280\n1.0459\nIPC (t-1)\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\n\n\n\n\n\n\n\n\nLa relation de long terme pour le modèle d’importation est :\n\\[\n\\begin{align}\n\\text{Importation}_t &= -0.5926 \\cdot \\text{Taux.Change}_t + 0.3108 \\cdot \\text{PIB}_t + 1.8199 \\cdot \\text{PIB.USA}_t + 1.0459 \\cdot \\text{IPC}_t + \\text{constante}\n\\end{align}\n\\]"
  },
  {
    "objectID": "specification_test_et.html",
    "href": "specification_test_et.html",
    "title": "Test de Spécification (Modèle ET)",
    "section": "",
    "text": "Le test RESET de Ramsey est un test de spécification de modèle. Il est utilisé pour détecter les erreurs de spécification, telles que les variables omises et la forme fonctionnelle incorrecte.\n\n\n\nH0 : Le modèle est correctement spécifié.\nH1 : Le modèle n’est pas correctement spécifié.\n\n\n\n\nLe tableau ci-dessous présente le résultat du test de Ramsey pour le modèle d’exportation (ET)."
  },
  {
    "objectID": "specification_test_et.html#test-de-spécification-de-ramsey-reset-pour-le-modèle-et",
    "href": "specification_test_et.html#test-de-spécification-de-ramsey-reset-pour-le-modèle-et",
    "title": "Test de Spécification (Modèle ET)",
    "section": "",
    "text": "Le test RESET de Ramsey est un test de spécification de modèle. Il est utilisé pour détecter les erreurs de spécification, telles que les variables omises et la forme fonctionnelle incorrecte.\n\n\n\nH0 : Le modèle est correctement spécifié.\nH1 : Le modèle n’est pas correctement spécifié.\n\n\n\n\nLe tableau ci-dessous présente le résultat du test de Ramsey pour le modèle d’exportation (ET)."
  },
  {
    "objectID": "diagnostic_tests.html",
    "href": "diagnostic_tests.html",
    "title": "Tests de Diagnostic des modeles",
    "section": "",
    "text": "Les tests de diagnostic sont essentiels pour vérifier la validité des hypothèses sous-jacentes d’un modèle économétrique. Ces tests permettent de s’assurer que le modèle est bien spécifié et que les résultats sont fiables.\n\n\nLe test de Breusch-Pagan est utilisé pour détecter l’hétéroscédasticité dans les résidus d’un modèle de régression. L’hétéroscédasticité signifie que la variance des erreurs n’est pas constante.\n\nH0 : Homoscédasticité (la variance des erreurs est constante).\nH1 : Hétéroscédasticité (la variance des erreurs n’est pas constante).\n\n\n\n\nLe test de Breusch-Godfrey est utilisé pour détecter l’autocorrélation des erreurs dans un modèle de régression. L’autocorrélation signifie que les erreurs sont corrélées entre elles.\n\nH0 : Il n’y a pas d’autocorrélation des erreurs.\nH1 : Il y a une autocorrélation des erreurs.\n\n\n\n\nLe test de Ljung-Box est un test statistique qui permet de déceler l’autocorrélation dans une série temporelle.\n\nH0 : Les résidus sont indépendants (pas d’autocorrélation).\nH1 : Les résidus ne sont pas indépendants (autocorrélation).\n\n\n\n\nLe test de Shapiro-Wilk est un test de normalité des résidus. Il permet de vérifier si les erreurs suivent une distribution normale.\n\nH0 : Les résidus suivent une distribution normale.\nH1 : Les résidus ne suivent pas une distribution normale.\n\n\n\n\nLe test RESET (Regression Specification Error Test) de Ramsey est un test de spécification de modèle. Il permet de vérifier si le modèle est bien spécifié, c’est-à-dire s’il n’y a pas de variables omises ou de formes fonctionnelles incorrectes.\n\nH0 : Le modèle est bien spécifié.\nH1 : Le modèle n’est pas bien spécifié."
  },
  {
    "objectID": "diagnostic_tests.html#introduction",
    "href": "diagnostic_tests.html#introduction",
    "title": "Tests de Diagnostic des modeles",
    "section": "",
    "text": "Les tests de diagnostic sont essentiels pour vérifier la validité des hypothèses sous-jacentes d’un modèle économétrique. Ces tests permettent de s’assurer que le modèle est bien spécifié et que les résultats sont fiables.\n\n\nLe test de Breusch-Pagan est utilisé pour détecter l’hétéroscédasticité dans les résidus d’un modèle de régression. L’hétéroscédasticité signifie que la variance des erreurs n’est pas constante.\n\nH0 : Homoscédasticité (la variance des erreurs est constante).\nH1 : Hétéroscédasticité (la variance des erreurs n’est pas constante).\n\n\n\n\nLe test de Breusch-Godfrey est utilisé pour détecter l’autocorrélation des erreurs dans un modèle de régression. L’autocorrélation signifie que les erreurs sont corrélées entre elles.\n\nH0 : Il n’y a pas d’autocorrélation des erreurs.\nH1 : Il y a une autocorrélation des erreurs.\n\n\n\n\nLe test de Ljung-Box est un test statistique qui permet de déceler l’autocorrélation dans une série temporelle.\n\nH0 : Les résidus sont indépendants (pas d’autocorrélation).\nH1 : Les résidus ne sont pas indépendants (autocorrélation).\n\n\n\n\nLe test de Shapiro-Wilk est un test de normalité des résidus. Il permet de vérifier si les erreurs suivent une distribution normale.\n\nH0 : Les résidus suivent une distribution normale.\nH1 : Les résidus ne suivent pas une distribution normale.\n\n\n\n\nLe test RESET (Regression Specification Error Test) de Ramsey est un test de spécification de modèle. Il permet de vérifier si le modèle est bien spécifié, c’est-à-dire s’il n’y a pas de variables omises ou de formes fonctionnelles incorrectes.\n\nH0 : Le modèle est bien spécifié.\nH1 : Le modèle n’est pas bien spécifié."
  },
  {
    "objectID": "diagnostic_tests.html#résultats-des-tests",
    "href": "diagnostic_tests.html#résultats-des-tests",
    "title": "Tests de Diagnostic des modeles",
    "section": "Résultats des Tests",
    "text": "Résultats des Tests\nLes résultats des tests de diagnostic pour chaque modèle sont présentés dans les pages suivantes :\n\nMODELE IT\n\n\n\n\n\n\n\n\n\n\n\nmodele ET"
  },
  {
    "objectID": "metadata.html",
    "href": "metadata.html",
    "title": "Metadonnee du modele",
    "section": "",
    "text": "Periode : 1988-2022\n\n\ntx.change : Taux de Change Reel\nimp: Importation Haitienne\nexp: Exportation Haitienne\npib: Produit Interieur Brute Haitienne\npib.usa: Produit Interieur Brute des Etats Unis\nipc: L’indice des prix a la Consommation"
  },
  {
    "objectID": "metadata.html#origine-source-de-donnee",
    "href": "metadata.html#origine-source-de-donnee",
    "title": "Metadonnee du modele",
    "section": "",
    "text": "Periode : 1988-2022\n\n\ntx.change : Taux de Change Reel\nimp: Importation Haitienne\nexp: Exportation Haitienne\npib: Produit Interieur Brute Haitienne\npib.usa: Produit Interieur Brute des Etats Unis\nipc: L’indice des prix a la Consommation"
  },
  {
    "objectID": "metadata.html#note-sur-le-taux-de-change",
    "href": "metadata.html#note-sur-le-taux-de-change",
    "title": "Metadonnee du modele",
    "section": "Note sur le taux de change",
    "text": "Note sur le taux de change"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Donnee du modele",
    "section": "",
    "text": "tx.change : Taux de Change Reel\nimp: Importation Haitienne\nexp: Exportation Haitienne\npib: Produit Interieur Brute Haitienne\npib.usa: Produit Interieur Brute des Etats Unis\nipc: L’indice des prix a la Consommation"
  },
  {
    "objectID": "data.html#periode-1988-2022",
    "href": "data.html#periode-1988-2022",
    "title": "Donnee du modele",
    "section": "",
    "text": "tx.change : Taux de Change Reel\nimp: Importation Haitienne\nexp: Exportation Haitienne\npib: Produit Interieur Brute Haitienne\npib.usa: Produit Interieur Brute des Etats Unis\nipc: L’indice des prix a la Consommation"
  },
  {
    "objectID": "jacquesbera.html",
    "href": "jacquesbera.html",
    "title": "Test de Normalite des Variables sous etudes",
    "section": "",
    "text": "Test d’Hypothèse de Jarque-Bera :\nHypothèse Nulle (H₀) : Les données proviennent d’une distribution normale.\nHypothèse Alternative (H₁) : Les données ne proviennent pas d’une distribution normale.\nStatistique de Jarque-Bera :\nLa statistique de Jarque-Bera (JB) est définie comme suit :\n\\[JB = \\frac{n}{6} \\left( S^2 + \\frac{(K-3)^2}{4} \\right)\\]\nOù :\n\nn est la taille de l’échantillon.\nS est le coefficient d’asymétrie de l’échantillon.\nK est le coefficient d’aplatissement de l’échantillon.\n\nCette statistique suit une distribution du chi carré avec 2 degrés de liberté sous l’hypothèse nulle (H₀)."
  },
  {
    "objectID": "jacquesbera.html#jarque-bera-test",
    "href": "jacquesbera.html#jarque-bera-test",
    "title": "Test de Normalite des Variables sous etudes",
    "section": "",
    "text": "Test d’Hypothèse de Jarque-Bera :\nHypothèse Nulle (H₀) : Les données proviennent d’une distribution normale.\nHypothèse Alternative (H₁) : Les données ne proviennent pas d’une distribution normale.\nStatistique de Jarque-Bera :\nLa statistique de Jarque-Bera (JB) est définie comme suit :\n\\[JB = \\frac{n}{6} \\left( S^2 + \\frac{(K-3)^2}{4} \\right)\\]\nOù :\n\nn est la taille de l’échantillon.\nS est le coefficient d’asymétrie de l’échantillon.\nK est le coefficient d’aplatissement de l’échantillon.\n\nCette statistique suit une distribution du chi carré avec 2 degrés de liberté sous l’hypothèse nulle (H₀)."
  },
  {
    "objectID": "jacquesbera.html#litterature-du-test-de-jarque-bera",
    "href": "jacquesbera.html#litterature-du-test-de-jarque-bera",
    "title": "Test de Normalite des Variables sous etudes",
    "section": "Litterature du Test de Jarque Bera",
    "text": "Litterature du Test de Jarque Bera\nLe test de Jarque-Bera est un test statistique utilisé pour évaluer si un échantillon de données donné présente un coefficient d’asymétrie et un coefficient d’aplatissement qui sont approximativement distribués selon une loi normale, ce qui est une hypothèse courante dans de nombreuses techniques statistiques.\n\nComparaison de la statistique de test à la valeur critique :\n\nLa statistique de test de Jarque-Bera suit une distribution du chi carré avec 2 degrés de liberté sous l’hypothèse nulle. Par conséquent, vous comparez la statistique de test calculée à la valeur critique de la distribution du chi carré avec 2 degrés de liberté à votre niveau de signification choisi (par exemple, 0,05 ou 0,01).\n\nPrise de décision :\n\n\nSi la statistique de test calculée est supérieure à la valeur critique, vous rejetez l’hypothèse nulle, concluant que les données ne proviennent pas d’une distribution normale.\nSi la statistique de test calculée est inférieure ou égale à la valeur critique, vous ne rejetez pas l’hypothèse nulle, ce qui indique qu’il n’y a pas suffisamment de preuves pour conclure que les données ne proviennent pas d’une distribution normale."
  },
  {
    "objectID": "jacquesbera.html#rappel-sur-la-notion-de-p-value",
    "href": "jacquesbera.html#rappel-sur-la-notion-de-p-value",
    "title": "Test de Normalite des Variables sous etudes",
    "section": "Rappel sur la notion de P-Value",
    "text": "Rappel sur la notion de P-Value\nDans le cadre du test de Jarque-Bera, la p-valeur est une mesure cruciale pour interpréter les résultats du test. Voici comment interpréter la p-valeur :\nSi la p-valeur est inférieure au seuil de signification (α) :\nCela signifie que la probabilité d’observer les données (ou des données encore plus extrêmes) sous l’hypothèse nulle (que les données proviennent d’une distribution normale) est faible. Vous rejetez alors l’hypothèse nulle au niveau de signification α. En d’autres termes, vous avez suffisamment de preuves pour conclure que les données ne suivent pas une distribution normale en termes d’asymétrie et/ou d’aplatissement. Si la p-valeur est supérieure au seuil de signification (α) :\nCela signifie que la probabilité d’observer les données (ou des données encore plus extrêmes) sous l’hypothèse nulle est élevée. Vous ne rejetez pas l’hypothèse nulle au niveau de signification α. En d’autres termes, vous ne disposez pas de suffisamment de preuves pour conclure que les données ne suivent pas une distribution normale en termes d’asymétrie et/ou d’aplatissement. En résumé :\nUne p-valeur faible suggère des preuves en faveur du rejet de l’hypothèse nulle, indiquant que les données ne suivent probablement pas une distribution normale. Une p-valeur élevée suggère un manque de preuves pour rejeter l’hypothèse nulle, ce qui signifie que les données pourraient suivre une distribution normale. Il est important de choisir un seuil de signification approprié (α) avant d’interpréter la p-valeur. Les valeurs typiques pour α sont 0,05 ou 0,01, mais cela dépend souvent du contexte de l’analyse et des normes de l’industrie."
  },
  {
    "objectID": "jacquesbera.html#conclusion-du-test-de-jarque-bera-des-series",
    "href": "jacquesbera.html#conclusion-du-test-de-jarque-bera-des-series",
    "title": "Test de Normalite des Variables sous etudes",
    "section": "Conclusion du Test de Jarque Bera des series",
    "text": "Conclusion du Test de Jarque Bera des series\n\nOn ne peut rejeter l’hypothese nulle pour aucune des series parce que \\(p \\ge \\alpha, \\alpha = 0.05\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Accueil",
    "section": "",
    "text": "Bienvenue, Ce site présente les données et les résultats de notre modèle."
  },
  {
    "objectID": "index.html#accueil",
    "href": "index.html#accueil",
    "title": "Accueil",
    "section": "",
    "text": "Bienvenue, Ce site présente les données et les résultats de notre modèle."
  },
  {
    "objectID": "cusum_test_et.html",
    "href": "cusum_test_et.html",
    "title": "Test CUSUM (Modèle ET)",
    "section": "",
    "text": "Le test CUSUM (Cumulative Sum) est un test de stabilité des paramètres du modèle. Il est basé sur la somme cumulative des résidus récursifs.\n\n\nSi la ligne CUSUM reste dans les limites de confiance (les deux lignes rouges), les paramètres du modèle sont considérés comme stables dans le temps."
  },
  {
    "objectID": "cusum_test_et.html#test-cusum-pour-le-modèle-et",
    "href": "cusum_test_et.html#test-cusum-pour-le-modèle-et",
    "title": "Test CUSUM (Modèle ET)",
    "section": "",
    "text": "Le test CUSUM (Cumulative Sum) est un test de stabilité des paramètres du modèle. Il est basé sur la somme cumulative des résidus récursifs.\n\n\nSi la ligne CUSUM reste dans les limites de confiance (les deux lignes rouges), les paramètres du modèle sont considérés comme stables dans le temps."
  },
  {
    "objectID": "kendall_correlation.html",
    "href": "kendall_correlation.html",
    "title": "Test de Corrélation de Kendall",
    "section": "",
    "text": "Le coefficient de corrélation de rang de Kendall, communément appelé tau de Kendall (τ), est une statistique utilisée pour mesurer l’association ordinale entre deux quantités mesurées. Un test tau est un test d’hypothèse non paramétrique pour la dépendance statistique basé sur le coefficient tau.\nLe coefficient de Kendall est calculé comme suit:\n\\(\\tau = \\frac{n_c - n_d}{\\frac{1}{2}n(n-1)}\\)\noù: - \\(n_c\\) est le nombre de paires concordantes. - \\(n_d\\) est le nombre de paires discordantes. - \\(n\\) est le nombre d’observations.\nUne paire de points \\((x_i, y_i)\\) et \\((x_j, y_j)\\) est dite concordante si \\(x_i &lt; x_j\\) et \\(y_i &lt; y_j\\), ou si \\(x_i &gt; x_j\\) et \\(y_i &gt; y_j\\). Elle est dite discordante si \\(x_i &lt; x_j\\) et \\(y_i &gt; y_j\\), ou si \\(x_i &gt; x_j\\) et \\(y_i &lt; y_j\\).\nLa valeur de \\(\\tau\\) est comprise entre -1 et 1. Une valeur de 1 indique une corrélation parfaite positive, une valeur de -1 indique une corrélation parfaite négative et une valeur de 0 indique une absence de corrélation.\nDonc on peut proceder a un test de correlation de Kendall."
  },
  {
    "objectID": "specification_test_it.html",
    "href": "specification_test_it.html",
    "title": "Test de Spécification (Modèle IT)",
    "section": "",
    "text": "Le test RESET de Ramsey est un test de spécification de modèle. Il est utilisé pour détecter les erreurs de spécification, telles que les variables omises et la forme fonctionnelle incorrecte.\n\n\n\nH0 : Le modèle est correctement spécifié.\nH1 : Le modèle n’est pas correctement spécifié.\n\n\n\n\nLe tableau ci-dessous présente le résultat du test de Ramsey pour le modèle d’importation (IT)."
  },
  {
    "objectID": "specification_test_it.html#test-de-spécification-de-ramsey-reset-pour-le-modèle-it",
    "href": "specification_test_it.html#test-de-spécification-de-ramsey-reset-pour-le-modèle-it",
    "title": "Test de Spécification (Modèle IT)",
    "section": "",
    "text": "Le test RESET de Ramsey est un test de spécification de modèle. Il est utilisé pour détecter les erreurs de spécification, telles que les variables omises et la forme fonctionnelle incorrecte.\n\n\n\nH0 : Le modèle est correctement spécifié.\nH1 : Le modèle n’est pas correctement spécifié.\n\n\n\n\nLe tableau ci-dessous présente le résultat du test de Ramsey pour le modèle d’importation (IT)."
  },
  {
    "objectID": "logdata.html",
    "href": "logdata.html",
    "title": "Donnee du modele en Logarithme Neperien",
    "section": "",
    "text": "tx.change : Taux de Change Reel\nimp: Importation Haitienne\nexp: Exportation Haitienne\npib: Produit Interieur Brute Haitienne\npib.usa: Produit Interieur Brute des Etats Unis\nipc: L’indice des prix a la Consommation"
  },
  {
    "objectID": "logdata.html#periode-1988-2022-transformation-en-log",
    "href": "logdata.html#periode-1988-2022-transformation-en-log",
    "title": "Donnee du modele en Logarithme Neperien",
    "section": "",
    "text": "tx.change : Taux de Change Reel\nimp: Importation Haitienne\nexp: Exportation Haitienne\npib: Produit Interieur Brute Haitienne\npib.usa: Produit Interieur Brute des Etats Unis\nipc: L’indice des prix a la Consommation"
  }
]